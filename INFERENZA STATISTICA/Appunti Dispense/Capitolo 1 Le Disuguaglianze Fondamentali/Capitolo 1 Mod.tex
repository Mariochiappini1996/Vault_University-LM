\documentclass[article,a4paper]{article}

% --- PACHETTI ESSENZIALI ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % Migliore gestione degli accenti
\usepackage[italian]{babel}
\usepackage[a4paper, margin=1in]{geometry} % Imposta i margini della pagina

% --- PACHETTI PER LA MATEMATICA ---
\usepackage{amsmath}
\usepackage{amssymb} % Per \mathbb
\usepackage{amsthm} % Per gli ambienti di proposizione e proof

% --- PACCHETTO PER LE SPIEGAZIONI (TCOLORBOX) ---
\usepackage[skins,breakable]{tcolorbox}

% --- DEFINIZIONE DEGLI AMBIENTI ---
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}

% --- STILE PER LE SCATOLE DI SPIEGAZIONE ---
% Definiamo un nuovo ambiente tcolorbox chiamato 'spiegazione'
\tcbset{
    spiegazionestyle/.style={
        colback=gray!10, % Sfondo grigio chiaro
        colframe=gray!60, % Bordo grigio scuro
        fonttitle=\bfseries,
        title=Spiegazione Semplice, % <-- EMOJI RIMOSSO
        sharp corners,
        boxsep=5pt,
        left=5pt,
        right=5pt,
        top=5pt,
        bottom=5pt,
        breakable, % Permette al box di spezzarsi tra le pagine
    }
}
% Creiamo il comando \begin{spiegazione} ... \end{spiegazione}
\newtcolorbox{spiegazione}{spiegazionestyle}


% --- INFORMAZIONI SUL DOCUMENTO ---
\title{APPUNTI DI STATISTICA MATEMATICA \\ \large (con note e spiegazioni aggiuntive)}
\author{Domenico Marinucci \\ \small Dipartimento di Matematica, Università di Roma Tor Vergata}
\date{December 12, 2024}


% --- INIZIO DEL DOCUMENTO ---
\begin{document}

\maketitle

\begin{abstract}
Note di un corso di Statistica Matematica, con particolare enfasi sulla
Teoria Asintotica
\end{abstract}

\section{Disuguaglianze Fondamentali}
In questo capitolo richiamiamo le disuguaglianze fondamentali che costituiscono
gli strumenti principali per lo sviluppo di tutta la teoria asintotica.

\begin{proposition}[Disuguaglianza di Markov]
Sia $X:\Omega\rightarrow\mathbb{R}$ una variabile
aleatoria a valori non-negativi (cioè $X(\omega)\ge0$ con probabilità 1) e con valor
medio finito (cioè $\mathbb{E}[|X|]=\mathbb{E}[X]<\infty).$ Allora $\forall t>0$ abbiamo
\[
\mathbb{P}(X\ge t)\le\frac{\mathbb{E}[X]}{t}
\]
\end{proposition}

\begin{spiegazione}
    \textbf{Cosa dice in parole povere?}
    Questa disuguaglianza fissa un limite massimo alla probabilità che un evento "raro" accada, basandosi solo sul valore medio.

    \begin{itemize}
        \item \textbf{$X:\Omega\rightarrow\mathbb{R}$}: Questa è la notazione formale per "X è una variabile aleatoria", cioè qualcosa che può assumere diversi valori numerici (es. il risultato del lancio di un dado, l'altezza di una persona).
        \item \textbf{$\mathbb{E}[X]$}: È il \textbf{valore medio} (o "valore atteso") di X. È la media di tutti i possibili risultati, pesata per la loro probabilità.
        \item \textbf{$\mathbb{P}(X\ge t)$}: È la \textbf{probabilità} che la variabile $X$ assuma un valore \textit{maggiore o uguale} a $t$.
    \end{itemize}

    \textbf{Esempio pratico:}
    Supponiamo che il \textit{valore medio} ($\mathbb{E}[X]$) dello stipendio annuale in una città sia 30.000€. Qual è la probabilità ($\mathbb{P}$) che una persona a caso guadagni \textit{almeno} ($ \ge $) 150.000€ ($t$)?

    Markov dice: $\mathbb{P}(Stipendio \ge 150.000) \le \frac{30.000}{150.000} = 0.2$.
    C'è al massimo il 20\% di probabilità. È un limite "lento", spesso non molto preciso, ma funziona sempre (purché $X$ sia non-negativa).
\end{spiegazione}


\begin{proof}
E' sufficiente osservare che
\begin{align*}
\mathbb{P}(X\ge t) &= \mathbb{E}[\mathbb{I}_{[t,\infty)}(X)]=\int_{t}^{\infty}dF_{X}(x) \\
&\le\int_{t}^{\infty}\frac{x}{t}dF_{X}(x)\le\frac{1}{t}\int_{0}^{\infty}xdF_{X}(x) \\
&=\frac{\mathbb{E}[X]}{t}
\end{align*}
\end{proof}

\begin{spiegazione}
    \textbf{Decodifichiamo la dimostrazione:}
    \begin{itemize}
        \item $\mathbb{I}_{[t,\infty)}(X)$: Si chiama "funzione indicatrice". È molto semplice: vale 1 se $X \ge t$ ed è 0 in caso contrario. Il suo valore medio, $\mathbb{E}[\mathbb{I}_{...}]$, è (per definizione) la probabilità che $X \ge t$.
        \item $\int_{t}^{\infty}dF_{X}(x)$: Questo integrale è il modo formale per "sommare" (o meglio, integrare) le probabilità di tutti i valori di $x$ che sono maggiori o uguali a $t$. È la stessa cosa di $\mathbb{P}(X \ge t)$.
        \item $\int_{t}^{\infty}\frac{x}{t}dF_{X}(x)$: Qui sta il trucco. Dato che stiamo guardando solo valori dove $x \ge t$, è sempre vero che $\frac{x}{t} \ge 1$. Sostituendo 1 con una quantità più grande ($\frac{x}{t}$), stiamo \textit{aumentando} il valore dell'integrale, da cui il segno $\le$.
    \end{itemize}
    Il resto è solo algebra per far riapparire la definizione di $\mathbb{E}[X]$.
\end{spiegazione}


\begin{proposition}[Disuguaglianza di Markov generalizzata]
Sia $g:\mathbb{R}\rightarrow\mathbb{R}^{+}$ crescente e X: $\Omega \rightarrow \mathbb{R}$ una variabile aleatoria tale che $\mathbb{E}[g(X)]<\infty$.
Allora
\[
Pr(X\ge t)\le\frac{\mathbb{E}[g(X)]}{g(t)}
\]
\end{proposition}

\begin{spiegazione}
    Questa è la versione "potenziata" di Markov. L'idea è: invece di usare $X$, usiamo una sua trasformazione $g(X)$ (purché $g$ sia una funzione crescente, come $g(x)=x^2$ o $g(x)=e^x$).

    Perché farlo? Perché scegliendo una funzione $g$ "intelligente", possiamo ottenere un limite (un "tappo" massimo) molto più preciso (più basso) per la nostra probabilità.
\end{spiegazione}

\begin{proof}
La dimostrazione è praticamente identica a quella già vista della disuguaglianza di Markov. In particolare, chiamando $Y=g(X)$ otteniamo
\begin{align*}
\mathbb{P}(X\ge t) &= \mathbb{P}(g(X)\ge g(t))=\mathbb{E}[\mathbb{I}_{[g(t),\infty)}(Y)] \\
&= \int_{g(t)}^{\infty}dF_{Y}(y)\le\int_{g(t)}^{\infty}\frac{Y}{g(t)}dF_{Y}(y) \\
&= \int_{t}^{\infty}\frac{g(x)}{g(t)}dF_{X}(x)\le\frac{1}{g(t)}\int_{0}^{\infty}g(x)dF_{X}(x) \\
&= \frac{\mathbb{E}[g(X)]}{g(t)}
\end{align*}
\end{proof}

\begin{remark}
La disuguaglianza di Markov e la sua versione generalizzata sono
il primo esempio di disuguaglianze di concentrazione;
in pratica, implicano che
tanto più una variabile aleatoria ammette momenti finite, tanto più le code
della sua distribuzione vanno a zero velocemente.
Ad esempio, consideriamo
una variabile aleatoria che ammetta funzione generatrice dei momenti finita in
un intorno dell'origine, cioè tale per cui
\[
m_{X}(u):=\mathbb{E}[\exp(uX)]<\infty \quad \text{per } u\in[0,T], T>1.
\]
Segue immediatamente che
\[
Pr(X\ge t)\le\frac{\mathbb{E}[\exp(X)]}{\exp(t)}=\frac{m_{X}(1)}{\exp(t)}
\]
cioè le code della distribuzione di X devono decadere almeno esponenzialmente.
\end{remark}

\begin{spiegazione}
    \textbf{Momenti e Code:}
    \begin{itemize}
        \item \textbf{Code (Tails)}: Sono le parti estreme di una distribuzione di probabilità, cioè la probabilità di ottenere valori molto alti o molto bassi (molto lontani dalla media).
        \item \textbf{Momenti (Moments)}: Sono misure che descrivono la forma della distribuzione.
            \begin{itemize}
                \item Il 1° momento è la \textbf{media} ($\mathbb{E}[X]$).
                \item Il 2° momento è legato alla \textbf{varianza} ($\mathbb{E}[X^2]$).
                \item E così via...
            \end{itemize}
    \end{itemize}
    Questo remark dice: se una variabile ha "momenti finiti" (cioè la sua media, varianza, ecc. non sono infinite), allora le sue "code vanno a zero velocemente". In pratica, è molto improbabile ottenere valori estremi.
    La \textbf{funzione generatrice dei momenti} ($m_X(u)$) è uno strumento matematico che "impacchetta" tutte le informazioni su tutti i momenti in un'unica funzione.
\end{spiegazione}


\begin{proposition}[Disuguaglianza di Chebyshev]
Sia $X:\Omega\rightarrow\mathbb{R}$ una variabile
aleatoria con momento secondo finito (cioè $\mathbb{E}[X^{2}]<\infty)$. Allora $\forall t>0$ abbiamo
\[
\mathbb{P}(|X-\mathbb{E}[X]|\ge t)\le\frac{Var[X]}{t^{2}}
\]
\end{proposition}

\begin{spiegazione}
    Questa è una delle disuguaglianze più famose ed è un caso specifico di Markov (generalizzata, usando $g(x) = (x-\mu)^2$).

    \textbf{Cosa dice in parole povere?}
    Ci dà un limite alla probabilità di "allontanarsi" dalla media, basandosi sulla \textit{varianza}.

    \begin{itemize}
        \item \textbf{$Var[X]$}: È la \textbf{Varianza}, una misura di "quanto è dispersa" la variabile $X$. Se $Var[X]$ è piccola, i valori di $X$ sono quasi tutti vicini alla media. Se è grande, i valori sono molto sparpagliati.
        \item \textbf{$|X-\mathbb{E}[X]|\ge t$}: È la notazione per "la distanza tra $X$ e la sua media $\mathbb{E}[X]$ è maggiore o uguale a $t$".
    \end{itemize}

    \textbf{Esempio pratico:}
    Se l'altezza media ($\mathbb{E}[X]$) è 175cm e la Varianza ($Var[X]$) è $25cm^2$, qual è la probabilità di trovare una persona più alta di 185cm o più bassa di 165cm?
    Qui $t=10cm$ (la distanza da 175).
    
    Chebyshev dice: $\mathbb{P}(|Altezza - 175| \ge 10) \le \frac{25}{10^2} = \frac{25}{100} = 0.25$.
    C'è al massimo il 25\% di probabilità. È un limite molto più "stretto" (preciso) di quello che darebbe Markov.
\end{spiegazione}


\begin{proof}
E' sufficiente osservare che, utilizzando la disuguaglianza di Markov per
$Y=|X-\mathbb{E}[X]|^{2}$
\[
\mathbb{P}(|X-\mathbb{E}[X]|\ge t)=\mathbb{P}(|X-\mathbb{E}[X]|^{2}\ge t^{2})
=\frac{\mathbb{E}[|X-\mathbb{E}[X]|^{2}]}{t^{2}}
\]
\end{proof}

\begin{remark}
Come discusso nei cenni storici più avanti, la disuguaglianza di
Chebyshev è precedente a quella di Markov;
in effetti Markov è stato un alunno
di Chebyshev a San Pietroburgo.
\end{remark}

\begin{example}[Legge debole dei grandi numeri]
Anticipando un poco la discussione nei prossimi capitoli, possiamo illustrare immediatamente l'applicazione
più importante della disuguaglianza di Chebyshev.
Siano infatti $X_{i}:\Omega\rightarrow\mathbb{R}$
variabili aleatorie indipendenti con momento secondo finito, e definiamo come
al solito valor medio e varianza $\mu:=\mathbb{E}[X]$, $\sigma^{2}:=\mathbb{E}[(X-\mu)^{2}]$.
Introduciamo
altresì la media aritmetica $\overline{X}_{n}:=\frac{1}{n}\sum_{i=1}^{n}X_{i};$ dalla disuguaglianza di Chebyshev
abbiamo immediatamente che, per ogni $\epsilon>0$
\[
Pr\{|\overline{X}_n - \mu| > \epsilon\} \le \frac{Var\{\overline{X}_n\}}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \rightarrow 0 \quad \text{per } n\rightarrow\infty.
\]
E' immediato verificare che l'ipotesi di indipendenza può essere generalizzata
a quella di incorrelazione;
anche le ipotesi di identica distribuzione può essere
facilmente abbandonata, come vedremo nei prossimi capitoli.
\end{example}

\begin{spiegazione}
    Questo è il cuore della statistica: \textbf{La Legge dei Grandi Numeri}.

    \begin{itemize}
        \item $\overline{X}_{n}$: È la \textbf{media campionaria}, cioè la media aritmetica calcolata su $n$ osservazioni (es. la media dei voti di 10 esami).
        \item $\mu$: È la \textbf{media vera} (o "media della popolazione") (es. la media di tutti i voti che prenderesti se potessi dare l'esame infinite volte).
    \end{itemize}

    \textbf{Cosa dice la formula?}
    La formula $Pr\{...\} \le \frac{\sigma^2}{n\epsilon^2}$ ci dice che la probabilità che la nostra "media campionaria" ($\overline{X}_n$) sia diversa dalla "media vera" ($\mu$) per più di un piccolo errore $\epsilon$, diminuisce man mano che $n$ (la dimensione del nostro campione) aumenta.

    Quando $n \rightarrow \infty$ (raccogliamo tantissimi dati), il termine $\frac{\sigma^2}{n\epsilon^2}$ va a 0.
    
    \textbf{In pratica:} Più dati raccogli ($n$ grande), più puoi essere sicuro che la media che hai calcolato ($\overline{X}_n$) sia vicina alla media vera ($\mu$). Questo è il motivo per cui i sondaggi funzionano (e funzionano meglio con più intervistati).
\end{spiegazione}


Nelle precedenti disuguaglianze abbiamo mostrato che per variabili aleatorie che ammettono un numero di momenti maggiore le code della funzione di
distribuzione decadono più velocemente.
E' pertanto naturale chiedersi se si pos-
sano ottenere risultati più stringenti focalizzandosi su classi di variabili aleatorie
che abbiano supporto compatto, risultando quindi uniformemente limitate.
La
risposta è affermativa, come mostrato nel prossimo risultato.

\begin{proposition}[Disuguaglianza di Hoeffding]
Sia $X_{1},...,X_{n}$ una successione
di variabili aleatorie indipendenti e tali che esistano due sequenze di numeri
reali $a_{i}$, $b_{i}$ $i=1,2,..,n$ per cui valga $a_{i}\le X_{i}\le b_{i}$ per ogni i;
assumiamo inoltre
che $\mathbb{E}[Y_{i}]=0$ per $i=1,...,n.$ Per ogni $t>0$ vale la disuguaglianza
\[
Pr\{\sum_{i=1}^{n}X_{i}\ge t\}\le \inf_{u\ge0}\{e^{-ut}\prod_{i=1}^{n}e^{u^{2}(b_{i}-a_{i})^{2}/8}\}
\]
\end{proposition}

\begin{spiegazione}
    \textbf{Cosa aggiunge Hoeffding?}
    Markov e Chebyshev funzionano per quasi tutte le variabili. Hoeffding funziona per una classe specifica: variabili che sono \textbf{limitate} ("supporto compatto"), cioè i loro valori sono \textit{sempre} contenuti in un intervallo $[a_i, b_i]$.

    \textbf{Esempio:} Il lancio di una moneta ($X_i$ può essere solo 0 o 1, quindi è limitato a [0, 1]). Il voto di un esame (limitato a [18, 30]).
    L'altezza di una persona \textit{non} è (teoricamente) limitata.

    Usando questa informazione extra (i limiti $a_i$ e $b_i$), Hoeffding fornisce un limite (un "tappo") \textit{esponenzialmente} più preciso (più basso) rispetto a Chebyshev.
\end{spiegazione}


\begin{proof}
Per la disuguaglianza di Markov abbiamo, quale che sia $u>0$
\begin{align*}
Pr\{\sum_{i=1}^{n}X_{i}\ge t\} &= Pr\{\sum_{i=1}^{n}uX_{i}\ge ut\} \\
&= Pr\{e^{u\sum_{i=1}^{n}X_{i}}\ge e^{ut}\} \\
&\le e^{-ut}\mathbb{E}[e^{u\sum_{i=1}^{n}X_{i}}] \\
&= e^{-ut}\prod_{i=1}^{n}\mathbb{E}[e^{uX_{i}}] .
\end{align*}
Scriviamo ora
$Y_{i}=\alpha b_{i}+(1-\alpha)a_{i}$, $\alpha=\frac{Y_{i}-a_{i}}{b_{i}-a_{i}}$
per la convessità della funzione esponenziale, si ha che
\begin{align*}
e^{uX_{i}} &\le \frac{Y_{i}-a_{i}}{b_{i}-a_{i}}e^{ub_{i}}+(1-\frac{Y_{i}-a_{i}}{b_{i}-a_{i}})e^{ua_{i}} \\
&= \frac{Y_{i}-a_{i}}{b_{i}-a_{i}}e^{ub_{i}}+\frac{b_{i}-Y_{i}}{b_{i}-a_{i}}e^{ua_{i}}
\end{align*}
da cui, ricordando che $\mathbb{E}[Y_{i}]=0$
\[
\mathbb{E}[e^{uX_{i}}]\le-\frac{a_{i}}{b_{i}-a_{i}}e^{ub_{i}}+\frac{b_{i}}{b_{i}-a_{i}}e^{ua_{i}}
\]
Scriviamo ora
\[
-\frac{a_{i}}{b_{i}-a_{i}}e^{ub_{i}}+\frac{b_{i}}{b_{i}-a_{i}}e^{ua_{i}}=e^{g_{i}(v_{i})}
\]
dove
\[
g_{i}(v_{i}):=-\gamma_{i} v_{i} + \log(1-\gamma_{i}+\gamma_{i}e^{v_i}), \quad \gamma_{i}=\frac{-a_i}{b_i-a_i}, \quad v_i = u(b_i-a_i);
\]
notiamo infatti che
\begin{align*}
e^{g_{i}(v_{i})} &= e^{-\gamma_{i}v_{i}}(1-\gamma_{i}+\gamma_{i}e^{v_{i}}) \\
&= e^{ua_{i}}(1+\frac{a_{i}}{b_{i}-a_{i}}-\frac{a_{i}}{b_{i}-a_{i}}e^{u(b_{i}-a_{i})}) \\
&= (\frac{b_{i}-a_{i}}{b_{i}-a_{i}}e^{ua_{i}}+\frac{a_{i}}{b_{i}-a_{i}}e^{ua_{i}}-\frac{a_{i}}{b_{i}-a_{i}}e^{ub_{i}}) \\
&= \frac{b_{i}}{b_{i}-a_{i}}e^{ua_{i}}-\frac{a_{i}}{b_{i}-a_{i}}e^{ub_{i}}.
\end{align*}
Notiamo che $g_{i}(0)=0$ ed inoltre
\[
g_{i}^{\prime}(0)=-\gamma_{i}+\frac{\gamma_{i}e^{v_{i}}}{(1-\gamma_{i}+\gamma_{i}e^{v_{i}})}|_{v_{i}=0}=0 ,
\]
\[
g_{i}^{\prime \prime}(v_i) = \frac{\gamma_i e^{v_i}(1-\gamma_i+\gamma_i e^{v_i}) - \gamma_i e^{v_i}(\gamma_i e^{v_i})}{(1-\gamma_i+\gamma_i e^{v_i})^2} = \frac{\gamma_i (1-\gamma_i) e^{v_i}}{(1-\gamma_i+\gamma_i e^{v_i})^2}
\]
Infatti, poiché $v>0$
\[
g_{i}^{\prime \prime}(v_i) = \frac{\gamma_i (1-\gamma_i)}{( (1-\gamma_i)e^{-v_i/2} + \gamma_i e^{v_i/2} )^2} \le \frac{\gamma_i(1-\gamma_i)}{(1-\gamma_i+\gamma_i)^2} \le \frac{1}{4}
\]
(l'ultima disuguaglianza segue da $\gamma_i(1-\gamma_i) \le 1/4$ per ogni $\gamma_i$).

Per il teorema del valor medio di Lagrange, esiste $\xi\in(0,v_{i})$ tale per cui
\[
g_{i}(v_{i})=g_{i}(0)+g_{i}^{\prime}(0)v_{i}+g_{i}^{\prime\prime}(\xi)\frac{v_{i}^{2}}{2}
=g_{i}^{\prime\prime}(\xi)\frac{v_{i}^{2}}{2}\le\frac{u^{2}(b_{i}-a_{i})^{2}}{8}.
\]
Ne segue che
\[
\mathbb{E}[e^{uX_i}] \le e^{g_i(v_i)} \le e^{u^2(b_i-a_i)^2/8},
\]
ed la Proposizione è dimostrata.
\end{proof}

\begin{corollary}[Media aritmetica di variabili aleatorie di Bernoulli]
Siano $Y_{1},...,Y_{n}$
variabili aleatorie indipendenti identicamente distribuite con legge di Bernoulli
Ber(p).
Per ogni $\epsilon>0$, abbiamo che
\[
Pr\{|\overline{Y}_{n}-p|>\epsilon\}\le2e^{-2n\epsilon^{2}}.
\]
\end{corollary}

\begin{spiegazione}
    Questo è un risultato pratico importantissimo che deriva da Hoeffding.
    \begin{itemize}
        \item \textbf{Variabile di Bernoulli (Ber(p))}: È il modello matematico per un singolo evento che ha due soli risultati. Es. "lancio di una moneta" (Testa/Croce) o "un utente clicca/non clicca". $p$ è la probabilità di successo (es. $p=0.5$ per una moneta onesta).
        \item \textbf{$\overline{Y}_{n}$}: È la media campionaria, che qui rappresenta la "frequenza" di successi (es. 60 Teste su 100 lanci, $\overline{Y}_n = 0.6$).
        \item \textbf{$p$}: È la probabilità vera (es. $p=0.5$).
    \end{itemize}
    
    \textbf{Cosa dice la formula?}
    Ci dà la probabilità che la nostra \textit{frequenza} misurata ($\overline{Y}_n$) sia diversa dalla \textit{probabilità} vera ($p$) per più di un errore $\epsilon$.
    
    Il termine $e^{-2n\epsilon^{2}}$ è importantissimo: ci dice che questa probabilità di errore \textbf{crolla esponenzialmente} all'aumentare di $n$ (numero di lanci).
\end{spiegazione}


\begin{proof}
Si prenda $X_{i}:=\frac{1}{n}(Y_{i}-p)$. E' immediato verificato che $\mathbb{E}[X_{i}]=0$ e
$a_{i}\le X_{i}\le b_{i}$ per ogni $i=1,...,n$, con $a_{i}=-\frac{p}{n}$ $b_{i}=\frac{1-p}{n}.$ da cui $(b_{i}-a_{i})^{2}=\frac{1}{n^{2}}$
Applicando la disuguaglianza di Hoeffding si ha
\[
Pr\{\overline{Y}_{n}-p>\epsilon\} \le \inf_{u}\{e^{-u\epsilon}\prod_{i=1}^{n}e^{u^{2}/8n^{2}}\} = \inf_{u}\{e^{-u\epsilon}e^{u^{2}/8n}\}
\]
e prendendo $u=4n\epsilon$ otteniamo
\[
\inf_{u}\{e^{-u\epsilon}e^{u^{2}/8n}\}\le e^{-4n\epsilon\times\epsilon}e^{16n^{2}\epsilon^{2}/8n}=e^{-2n\epsilon^{2}}.
\]
La dimostrazione è completata ripetendo lo stesso ragionamento per $Pr \{\overline{Y}_{n}-p<-\epsilon\}$.
\end{proof}

\begin{remark}[Un confronto tra la disuguaglianza di Chebyshev e quella di Hoeffding]
Nel caso di variabili aleatorie limitate, la disuguaglianza di Hoeffding
è enormemente più efficiente della disuguaglianza di Chebyshev.
Ad esempio,
nel caso di variabili Bernoulliane di parametro $p=\frac{1}{2}$ conn $n=100$ ed $\epsilon=0.2$ la
disuguaglianza di Chebyshev ci dà il seguente limite limite superiore:
\[
Pr\{|\overline{Y}_{n}-p|>0.2\}\le\frac{Var\{\overline{Y}_{n}\}}{\epsilon^{2}}=\frac{1}{4n\epsilon^{2}}\simeq0.0625
\]
mentre con la disuguaglianza di Hoeffding si ottiene
\[
Pr\{|\overline{Y}_{n}-p|>0.2\}\le2e^{-2\times100\times(0.2)^{2}}\simeq0.00067.
\]
\end{remark}

\begin{spiegazione}
    Questo confronto è la chiave.
    Vogliamo sapere la probabilità che, dopo 100 lanci di una moneta onesta ($n=100, p=0.5$), la nostra frequenza di Teste sia "sbagliata" di molto (es. più di 0.7 o meno di 0.3, cioè $\epsilon=0.2$).

    \begin{itemize}
        \item \textbf{Chebyshev} (che usa solo la varianza) dice: "Questa probabilità è al massimo 6.25\%".
        \item \textbf{Hoeffding} (che usa anche il fatto che i lanci sono \textit{limitati} tra 0 e 1) dice: "Questa probabilità è al massimo 0.067\%".
    \end{itemize}
    Hoeffding è "enormemente più efficiente" perché sfrutta più informazioni sulla variabile.
\end{spiegazione}


\begin{remark}[Concentrazione del volume di un ipercubo]
La disuguaglianza
di Hoeffding ammette una interessante intepretazione geometrica quando viene
applicata al comportmanto de volume di un ipercubo $[0,1]^{n}$, nel limite in cui
$n\rightarrow\infty.$ Consideriamo infatti l'iperpiano $\Gamma_{n}:=\{x_{i}:x_{1}+x_{2}+...x_{n}=n/2\}$;
definitamo un "Tubo" di raggio $\epsilon$ intorno a $\Gamma_{n}$ come
\[
Tub_{\epsilon}(\Gamma_{n}):=\{x\in\mathbb{R}^{n}:dist\{x,\Gamma_{n}\}\le\epsilon\}.
\]
Per ogni $\epsilon>0,$ il volume dell'intersezione tra $Tub_{\epsilon}(\Gamma_{n})$ e $[0,1]^{n}$ converge esponenzialmente a 1 quando n diverge all'infinito;
in altre parole, la massa del
cubo si concentra esponenzialmente in un intorno arbitrariamente piccolo della
diagonale principale.
\end{remark}

Le precedenti disuguaglianze ci permettono di avere dei limiti superiori per
il comportamento delle code di variabili aleatorie generiche.
E' interessante confrontare il loro comportamento con quello delle code di una variabile Gaussiana
standard;
a questo scopo introduciamo la prossima proposizione.

\begin{proposition}[Disuguaglianza di Mill-Gordon]
Sia $Z\sim N(0,1)$ una variabile Gaussiana standard. Si ha che
\[
\sqrt{\frac{1}{2\pi}}\frac{t}{1+t^{2}}\exp(-\frac{t^{2}}{2})\le Pr\{Z>t\}\le\sqrt{\frac{1}{2\pi}}\frac{1}{t}\exp(-\frac{t^{2}}{2}) .
\]
\end{proposition}

\begin{spiegazione}
    Questa disuguaglianza è diversa. Non è "generale" come Markov o Chebyshev, ma si applica \textit{solo} alla \textbf{Variabile Gaussiana Standard} ($Z \sim N(0,1)$), la famosa "curva a campana".

    \textbf{Cosa dice?}
    Calcolare $Pr\{Z>t\}$ (l'area sotto la coda della campana) è difficile. Questa formula non dà un limite superiore "lento", ma "intrappola" la vera probabilità tra due valori molto vicini e facili da calcolare. È un modo molto efficiente per stimare le code della Gaussiana.
\end{spiegazione}


\begin{proof}
La disuguaglianza di destra può essere stabilita con un semplice cambio
di variabile; abbiamo:
\begin{align*}
Pr\{Z>t\} &= \int_{t}^{\infty}\frac{1}{\sqrt{2\pi}}\exp(-\frac{x^{2}}{2})dx \\
&\le\int_{t}^{\infty}\frac{x}{t}\frac{1}{\sqrt{2\pi}}\exp(-\frac{x^{2}}{2})dx \\
&= \frac{1}{t}\frac{1}{\sqrt{2\pi}}\int_{t^{2}/2}^{\infty}\exp(-y)dy \quad (\text{dopo il cambio di variabile } y=\frac{x^{2}}{2}) \\
&= \frac{1}{t}\frac{1}{\sqrt{2\pi}}\exp(-\frac{t^{2}}{2}) .
\end{align*}
Per il limite inferiore, possiamo ragionare
come segue:
\begin{align*}
Pr\{Z>t\} &= \int_{t}^{\infty}\frac{1}{\sqrt{2\pi}}\exp(-\frac{x^{2}}{2})dx \\
&\ge\int_{t}^{\infty}\frac{x^{4}+2x^{2}-1}{x^{4}+2x^{2}+1}\frac{1}{\sqrt{2\pi}}\exp(-\frac{x^{2}}{2})dx \\
&= \int_{t}^{\infty}\frac{x^{2}(x^{2}+1)+x^{2}-1}{(x^{2}+1)^{2}}\frac{1}{\sqrt{2\pi}}\exp(-\frac{x^{2}}{2})dx \\
&= \int_{t}^{\infty}\{\frac{x^{2}}{(x^{2}+1)} + \frac{x^{2}-1}{(x^{2}+1)^{2}}\}\frac{1}{\sqrt{2\pi}}\exp(-\frac{x^{2}}{2})dx
\end{align*}
Chiamiamo ora
\[
\psi(x)=\frac{x}{1+x^{2}}\exp(-\frac{x^{2}}{2})
\]
e notiamo che
\begin{align*}
\psi^{\prime}(x) &= \frac{d\psi(x)}{dx}=\frac{1(1+x^{2})-x(2x)}{(1+x^{2})^{2}}\exp(-\frac{x^{2}}{2}) + \frac{x}{1+x^{2}}\exp(-\frac{x^{2}}{2})(-x) \\
&= \frac{1-x^2}{(1+x^2)^2}\exp(-\frac{x^{2}}{2}) - \frac{x^2(1+x^2)}{(1+x^2)^2}\exp(-\frac{x^{2}}{2}) \\
&= \frac{1-x^2-x^2-x^4}{(1+x^2)^2}\exp(-\frac{x^{2}}{2}) = -\frac{x^4+2x^2-1}{(x^2+1)^2}\exp(-\frac{x^{2}}{2})
\end{align*}
(Nota: C'è un'apparente discrepanza tra la derivata $\psi'(x)$ e l'integrando. Seguendo la logica del testo, l'integrale della derivata deve restituire la funzione. L'integrando è $A+B$ dove $A=\frac{x^2}{(x^2+1)}\exp(-x^2/2)$ e $B = \frac{x^2-1}{(x^2+1)^2}\exp(-x^2/2)$. La derivata di $\psi(x)$ è $\psi'(x) = \frac{1-x^2}{(1+x^2)^2}\exp(-x^2/2) - \frac{x^2(1+x^2)}{(1+x^2)^2}\exp(-x^2/2)$.
L'espressione nel PDF [source 130] è:
\[
\psi^{\prime}(x) = \dots = -\{\frac{x^{2}}{(x^{2}+1)^{2}}+\frac{x^{2}-1}{(x^{2}+1)^{2}}\}\exp(-\frac{x^{2}}{2})
\]
Questa espressione non sembra corretta. La derivata corretta è:
\[
\psi^{\prime}(x) = \frac{1-x^2}{(1+x^2)^2}e^{-x^2/2} - \frac{x^2}{1+x^2}e^{-x^2/2} = \frac{(1-x^2) - x^2(1+x^2)}{(1+x^2)^2}e^{-x^2/2} = - \frac{x^4+2x^2-1}{(1+x^2)^2}e^{-x^2/2}
\]
L'integrando nel PDF [source 128] è $\frac{x^4+2x^2-1}{(x^2+1)^2} \frac{1}{\sqrt{2\pi}} \exp(-x^2/2) = -\psi'(x) / \sqrt{2\pi}$.
Quindi, la dimostrazione è completata dal Teorema Fondamentale del Calcolo:)
\[
\frac{1}{\sqrt{2\pi}}\int_{t}^{\infty} \left(-\psi^{\prime}(x)\right) dx = -\frac{1}{\sqrt{2\pi}}[\psi(x)]_{t}^{\infty} = -\frac{1}{\sqrt{2\pi}}[0 - \psi(t)] = \sqrt{\frac{1}{2\pi}}\frac{t}{1+t^{2}}\exp(-\frac{t^{2}}{2}) .
\]
\end{proof}

\begin{remark}
Queste due disuguglianze sono piuttosto efficienti; ad esempio, per
$t=2$ abbiamo
\[
Pr\{Z>2\}=\int_{2}^{\infty}\frac{1}{\sqrt{2\pi}}\exp(-\frac{x^{2}}{2})dx=0.02275
\]
mentre le stime di Mill danno
\[
\frac{1}{\sqrt{2\pi}}\frac{t}{1+t^{2}}\exp(-\frac{t^{2}}{2})|_{t=2}=0.021596\le Pr\{Z>2\}\le\frac{1}{\sqrt{2\pi}}\frac{1}{t}\exp(-\frac{t^{2}}{2})|_{t=2}=0.027
\]
Per $t=3$ il risulta diventa ancora più preciso; otteniamo
\[
Pr\{Z>3\}=\int_{3}^{\infty}\frac{1}{\sqrt{2\pi}}\exp(-\frac{x^{2}}{2})dx=0.001349
\]
mentre le stime di Mill danno
\begin{align*}
\frac{1}{\sqrt{2\pi}}\frac{t}{1+t^{2}}\exp(-\frac{t^{2}}{2})|_{t=3} &= 0.001329 \\
\le Pr\{Z>3\} &\le \frac{1}{\sqrt{2\pi}}\frac{1}{t}\exp(-\frac{t^{2}}{2})|_{t=3}=0.0014773 .
\end{align*}
\end{remark}

\begin{remark}
Vediamo quale risultato otterremo per la somma di variabili Bernoulliane considerando l'approssimazione asintotica che segue dal teorema del Limte
Centrale;
prendendo per semplicità $p=\frac{1}{2}$ abbiamo:
\[
\overline{Y}_{n}-\mathbb{E}[\overline{Y}_{n}]\simeq\mathcal{N}(0,\frac{p(1-p)}{n})=\mathcal{N}(0,\frac{1}{4n})
\]
ed utilizzando la disuguaglianza di Mill abbiamo dunque
\begin{align*}
Pr\{|\overline{Y}_{n}-\frac{1}{2}|>0.2\} &= Pr\{|\mathcal{N}(0,\frac{1}{4n})|>0.2\}+o_{n\rightarrow\infty}(1) \\
&= Pr\{|\mathcal{N}(0,1)|>0.2\times2\sqrt{n}\} \\
&\le\frac{2}{\sqrt{2\pi}}\frac{1}{0.2\times2\sqrt{n}}e^{-4\times n\times(0.2)^{2}/2}.
\end{align*}
Per $n=100$ otteniamo
\[
\frac{2}{\sqrt{2\pi}}\frac{1}{0.2\times20}e^{-2\times100\times(0.2)^{2}}=6.6915\times10^{-5};
\]
un valore circa 10 volte inferiore a quello ottenuto dalla disuguaglianza di Hoeffding.
Il risultato però non deve ingannare: la disuguaglianza di Hoeffding vale
in senso stretto, mentre qui stiamo trovando un limite superiore alla probabilità
Gaussiana, tralasciando il fatto che il termine di approssimazione $o_{n\rightarrow\infty}(1)$ è
molto più grande della maggiorazione ottenuta: si può dimostrare solo l'ordine
$O(n^{-1/2})$.
In altre parole, il Teorema del Limite Centrale NON può essere
sfruttato per ottenere un limite così efficiente per la probabilità sulle code della
variabile $\overline{Y}_{n}$
\end{remark}

La prossima disuguaglianza dovrebbe essere ben nota dai corsi di Geometria
ed Analisi.

\begin{proposition}[Disuguaglianza di Cauchy-Schwartz]
Siano $X,Y:\Omega\rightarrow\mathbb{R}$
variabili aleatorie con momento secondo finito. Abbiamo che
\[
|\mathbb{E}[XY]|^{2}\le\mathbb{E}[X^{2}]\mathbb{E}[Y^{2}].
\]
La disuguaglianza è stretta, a meno che le variabili aleatorie siano tra loro in
relazione lineare con probabilità 1, cioè esista un numero reale $t^{*}$ tale per cui
\[
\mathbb{E}[(Y-t^{*}X)^{2}]=0.
\]
\end{proposition}

\begin{spiegazione}
    Questa è l'equivalente della famosa disuguaglianza che si studia in geometria, ma applicata alle variabili aleatorie.
    
    \textbf{Spiegazione geometrica (Remark 15):}
    Si può pensare alle variabili aleatorie come a dei "vettori" in uno spazio astratto.
    \begin{itemize}
        \item $\mathbb{E}[X^2]$ è come il "quadrato della lunghezza" del vettore $X$.
        \item $\mathbb{E}[XY]$ è come il "prodotto scalare" tra i vettori $X$ e $Y$.
    \end{itemize}
    
    La disuguaglianza dice che il prodotto scalare al quadrato (che misura quanto sono "allineati") è al massimo il prodotto delle loro lunghezze al quadrato.
    
    L'uguaglianza vale solo se i vettori sono "allineati", cioè se $Y$ è un multiplo di $X$ ($Y = t^*X$). In statistica, questo significa che $X$ e $Y$ sono \textbf{perfettamente correlate linearmente}.
\end{spiegazione}


\begin{proof}
Consideriamo la funzione quadratica (in t)
\[
g(t):\mathbb{E}[(Y-tX)^{2}]=\mathbb{E}[Y^{2}]-2t\mathbb{E}[XY]+t^{2}\mathbb{E}[X^{2}]\ge0.
\]
L'equazione $g(t)=0$ ovviamente ammette al più una radice reale di molteplicità
due;
quindi il discriminante deve avere valore non-positivo, in particolare
\[
4(\mathbb{E}[XY])^{2}-4\mathbb{E}[X^{2}]\mathbb{E}[Y^{2}]\le0
\]
da cui segue la disuguaglianza, che diviene una ugualianza se e solo se esiste $t^{*}$
tale per cui $g(t^{*})=0.$
\end{proof}

\begin{remark}
Il richiamo alla geometria ci anticipa una idea che avrà grande
importanza nei corsi di probabilità più avanzati: lo spazio delle variabili aleatorie
di momento secondo finito può essere visto come uno spazio vettoriale dotato di
un proodtto interno (prodotto scalare) definito da
\[
\langle X,Y\rangle:=\mathbb{E}[XY].
\]
Nel caso particolare di variabili con valor medio nullo questo prodotto interno è
la covarianza.
\end{remark}

Per la prossima disuguaglianza ricordiamo innanzitutto che una funzione
$g(.):\mathbb{R}\rightarrow\mathbb{R}$ si dice convessa se per ogni $\alpha\in(0,1)$ e per ogni $x_{1}$ e $x_{2}$ nel suo
dominio (connesso), si ha che
\[
g(\alpha x_{1}+(1-\alpha)x_{2})\le\alpha g(x_{1})+(1-\alpha)g(x_{2}).
\]
Data una funzione convessa per ogni punto $x_{0}$ esiste una costante $L=L_{x_{0}}$ tale
per cui
\[
g(x)\ge g(x_{0})+L(x-x_{0});
\]
la costante non è necessariamente unica (se la funzione è derivabile, coincide
con il valore della derivata in quel punto).

\begin{proposition}[Disuguaglianza di Jensen]
Sia X una variabile aleatoria con
valor medio finito e $g(.)$ una funzione convessa.
Abbiamo che
\[
\mathbb{E}[g(X)] \ge g(\mathbb{E}[(X)]),
\]
dove la grandezza a sinistra della disuguaglianza può essere infinita.
\end{proposition}

\begin{spiegazione}
    \textbf{Cosa dice in parole povere?}
    Per una funzione \textbf{convessa} (una funzione a forma di "scodella" U, come $g(x)=x^2$), la media \textit{dopo} aver applicato la funzione è sempre maggiore o uguale della funzione applicata \textit{alla media}.

    \textbf{Esempio Pratico:}
    Prendiamo $X$ che vale -1 o +1 con la stessa probabilità (50/50), e usiamo la funzione convessa $g(x)=x^2$.
    
    \begin{itemize}
        \item \textbf{Calcoliamo $g(\mathbb{E}[X])$ (lato destro):}
        La media di $X$ è $\mathbb{E}[X] = (-1 \times 0.5) + (1 \times 0.5) = 0$.
        Applichiamo $g$: $g(\mathbb{E}[X]) = g(0) = 0^2 = \textbf{0}$.
        
        \item \textbf{Calcoliamo $\mathbb{E}[g(X)]$ (lato sinistro):}
        Applichiamo $g$ *prima*: $g(-1) = (-1)^2 = 1$; e $g(1) = 1^2 = 1$.
        Ora calcoliamo la media di $g(X)$: $\mathbb{E}[g(X)] = (1 \times 0.5) + (1 \times 0.5) = \textbf{1}$.
    \end{itemize}
    
    Come previsto dalla disuguaglianza, $1 \ge 0$.
    
    (Se la funzione fosse \textit{concava}, a forma di $\cap$ come $g(x)=\log(x)$, il segno $\ge$ si invertirebbe in $\le$).
\end{spiegazione}

\begin{proof}
E' sufficiente osservare che, per la monotonia del valor medio e prendendo $x_{0}=\mathbb{E}[(X)]$
\begin{align*}
\mathbb{E}[g(X)] &\ge \mathbb{E}[g(x_{0})+L(X-x_{0})] = \mathbb{E}[g(x_{0})]+\mathbb{E}[L(X-x_{0})] \\
&= g(\mathbb{E}[(X)])+L\mathbb{E}[(X-\mathbb{E}[(X)])]=g(\mathbb{E}[(X)]).
\end{align*}
\end{proof}

\end{document}