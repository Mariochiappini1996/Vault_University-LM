\documentclass[article,a4paper]{article}

% --- PACHETTI ESSENZIALI ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[italian]{babel}
\usepackage[a4paper, margin=1in]{geometry} % Imposta i margini della pagina

% --- PACHETTI PER LA MATEMATICA ---
\usepackage{amsmath}
\usepackage{amssymb} % Per \mathbb
\usepackage{amsthm} % Per gli ambienti di proposizione e proof
\usepackage{mathrsfs} % Per \mathfrak

% --- DEFINIZIONE DEGLI AMBIENTI ---
\newtheoremstyle{miostile}
  {\topsep} % space before
  {\topsep} % space after
  {\itshape} % body font
  {} % indent
  {\bfseries} % head font
  {.} % punctuation after head
  {.5em} % space after head
  {} % head spec
\theoremstyle{miostile}

% Imposta la numerazione per continuare dai capitoli precedenti
\newtheorem{definition}{Definition}
\newtheorem{example}[definition]{Example}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{remark}[definition]{Remark}
\setcounter{definition}{115} % L'ultimo era 115, quindi il prossimo è 116

% Rridefiniamo l'ambiente proof in italiano
\renewcommand{\proofname}{Proof}

% Comandi personalizzati
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}


% --- INIZIO DEL DOCUMENTO ---
\begin{document}

\section*{13 Statistiche sufficienti}
Una domanda naturale che possiamo porci, specialmente in un momento in cui
masse enormi di dati sono a disposizione, è la seguente: posso comprimere un
campione di dati osservati senza perdere informazione sul parametro che mi
interessa?
Questa domanda ci porta alla nozione di statistiche sufficienti.

\begin{definition}[Statistiche sufficienti]
Una statistica $T_{n}=T(X_{1},...,X_{n})$ si
dice sufficiente per il parametro $\theta$ se la distribuzione del campione condizionata
a $T_{n}$ non dipende da $\theta$.
In altre parole, la legge $p(X_{1},...,X_{n}|T_{n})$ non dipende dal parametro.
In
questa sezione, supponiamo sempre per semplicità di avere a che fare con variabili aleatorie discrete;
in questo ambito, è immediato verificare che conoscendo
solo la statistica sufficiente possiamo generare un nuovo campione uguale in distribuzione all'originale.
Abbiamo infatti, per qualsiasi scelta di valori $(x_{1},x_{2},...,x_{n})$
\begin{align*}
Pr(X_{1}=x_{1},...,X_{n}=x_{n}) \\
= Pr(X_{1}=x_{1},...,X_{n}=x_{n}\cap T(X_{1},...,X_{n})=T(x_{1},...,x_{n})) \\
= Pr(X_{1}=x_{1},...,X_{n}=x_{n}|T(X_{1},...,X_{n})=T(x_{1},...,x_{n})) Pr(T(X_{1},...,X_{n})=T(x_{1},...,x_{n})) \\
= Pr(Y_{1}=x_{1},...,Y_{n}=x_{n}|T(Y_{1},...,Y_{n})=T(x_{1},...,x_{n})) Pr(T(X_{1},...,X_{n})=T(x_{1},...,x_{n})) \\
= Pr(Y_{1}=x_{1},...,Y_{n}=x_{n})
\end{align*}
dove le $Y_{i}$ sono generate dalla distribuzione condizionata delle $X_{i}$ a T;
poichè
l'uguaglianza vale per tutte le scelte di $(x_{1},...,x_{n})$ abbiamo quindi dimostrato
che
\[
(Y_{1},...,Y_{n})\underline{d}(X_{1},...,X_{n}).
\]
Si noti che nella dimostrazione (primo passaggio) abbiamo anche utilizzato il
fatto che l'evento $X_{1}=x_{1},...,X_{n}=x_{n}$ è contenuto nell'evento $T(X_{1},...,X_{n})=$
$T(x_{1},...,x_{n})$, e pertanto la probabilità della loro intersezione è uguale alla probabilità di $X_{1}=x_{1},...,X_{n}=x_{n}$. Più in generale, denotiamo $q(T(X_{1},...,X_{n});\theta)$ la
funzione di probabilità della statistica sufficiente T;
dalla definizione di probabilità condizionata è immediato verificare che T è sufficiente se e solo se $p(X_{1},...,X_{n};\theta)/q(T(X_{1},...,X_{n});\theta)$
non dipende da $\theta$.
\end{definition}

\begin{example}
Si consideri un campione IID di variabili Bernoulliane $X_{i}\sim$
$Ber(\theta)$. La loro legge congiunta è data da
\[
p(X_{1},...,X_{n};\theta)=\theta^{\sum_{i=1}^{n}X_{i}}(1-\theta)^{n-\sum_{i=1}^{n}X_{i}}.
\]
Verifichiamo che $T_{n}:=\sum_{i=1}^{n}X_{i}$ sia una statistica sufficiente; la sua funzione
di probabilità è una binomiale, data quindi da
\[
q(T_{n}(X_{1},...,X_{n});\theta)=\binom{n}{\sum_{i=1}^{n}X_{i}}\theta^{\sum_{i=1}^{n}X_{i}}(1-\theta)^{n-\sum_{i=1}^{n}X_{i}}
\]
e quindi
\[
\frac{p(X_{1},...,X_{n};\theta)}{q(T_{n}(X_{1},...,X_{n});\theta)}=\frac{1}{\binom{n}{\sum_{i=1}^{n}X_{i}}}
\]
che è indipendente da $\theta$.
\end{example}

\begin{example}
Si consideri un campione IID di variabili Gaussiane $X_{i}\sim N(\theta,1).$
La loro legge congiunta è data da
\[
p(X_{1},...,X_{n};\mu)=\frac{1}{(2\pi)^{n/2}}exp(-\frac{1}{2}\sum_{i=1}^{n}(X_{i}-\mu)^{2}) ;
\]
consideriamo ora la statistica $\overline{X}_{n}=n^{-1}\sum_{i=1}^{n}X_{i}$ con legge $N(\mu,\frac{1}{n})$ cioè
\[
q(\overline{X}_{n};\mu)=\frac{\sqrt{n}}{(2\pi)^{1/2}}exp(-\frac{n}{2}(\overline{X}_{n}-\mu)^{2}) .
\]
E' facile verificare che
\[
\sum_{i=1}^{n}(X_{i}-\mu)^{2}=\sum_{i=1}^{n}(X_{i}-\overline{X}_{n}+\overline{X}_{n}-\mu)^{2}=\sum_{i=1}^{n}(X_{i}-\overline{X}_{n})^{2}+n(\overline{X}_{n}-\mu)^{2}
\]
da cui
\begin{align*}
\frac{p(X_{1},...,X_{n};\mu)}{q(\overline{X}_{n};\mu)} &= Const\times\frac{exp(-\frac{1}{2}\sum_{i=1}^{n}(X_{i}-\mu)^{2})}{exp(-\frac{n}{2}(\overline{X}_{n}-\mu)^{2})} \\
&= Const\times exp(-\frac{1}{2}\sum_{i=1}^{n}(X_{i}-\overline{X}_{n})^{2}) ,
\end{align*}
e la dimostrazione della sufficienza è completata.
\end{example}

A metà del ventesimo secolo Halmos e Savage hanno dato una caratterizzazione della suffiicenza leggermente più generale, come segue.

\begin{theorem}[Criterio di Fattorizzazione di Halmos e Savage]
Una statistica
è sufficiente se e solo se la funzione di probabilità (o di densità) di $X_{1},...,X_{n}$
si fattorizza come
\[
p(X_1, ..., X_n; \theta) = g(T(X_1,..., X_n);\theta))h(X_1,..., X_n)
\]
dove la funzione $h(.)$ non dipende da $\theta$.
\end{theorem}

\begin{proof}
L'implicazione sufficienza $\Rightarrow$ fattorizzazione è ovvia: basta prendere
per $g(T;\theta)=q(T;\theta)$ (la legge di q) e h la legge delle $X_{i}$ condizionata a T,
che sappiamo essere indipendente da $\theta$. L'interesse del Teorema è quindi nella
seconda parte, cioè nel caso in cui la fattorizzazione non corrisponda necessariamente alla coppia legge di T/ legge di $X_{1},...,X_{n}$ condizionata a T. Come
sempre in questa sezione ci limitiamo il caso discreto, e partizioniamo il dominio
di $X_{1},...,X_{n}$ in classi di equivalenza corrispondenti ai valori della statistica sufficiente;
definiamo cioè i sottoinsiemi di $\mathbb{R}^n$
\[
A_{T(x_{1},...,x_{n})}=\{(x_{1}^{\prime},...,x_{n}^{\prime}):T(x_{1}^{\prime},...,x_{n}^{\prime})=T(x_{1},...,x_{n})\}.
\]
Andiamo a calcolare, per $x_{1},...,x_{n}$ qualsiasi ma fissati
\begin{align*}
\frac{p(x_{1},...,x_{n};\theta)}{q(T(x_{1},...,x_{n});\theta)} &= \frac{g(T(x_{1},...,x_{n};\theta))h(x_{1},...,x_{n})}{q(T(x_{1},...,x_{n});\theta)} \\
&= \frac{g(T(x_{1},...,x_{n};\theta))h(x_{1},...,x_{n})}{\sum_{(x_{1}^{\prime},...,x_{n}^{\prime})\in A_{T(x_{1},...,x_{n})}}p(x_{1}^{\prime},...,x_{n}^{\prime};\theta)} \\
&= \frac{g(T(x_{1},...,x_{n};\theta))h(x_{1},...,x_{n})}{\sum_{(x_{1}^{\prime},...,x_{n}^{\prime})\in A_{T(x_{1},...,x_{n})}}g(T(x_{1},...,x_{n};\theta))h(x_{1}^{\prime},...,x_{n}^{\prime})} \\
&= \frac{h(x_{1},...,x_{n})}{\sum_{(x_{1}^{\prime},...,x_{n}^{\prime})\in A_{T(x_{1},...,x_{n})}}h(x_{1}^{\prime},...,x_{n}^{\prime})} ;
\end{align*}
l'ultima frazione non dipende da $\theta$, il che è sufficiente a concludere la dimostrazione della sufficienza.
\end{proof}

\subsection*{13.1 Il Teorema di Rao-Blackwell}
L'idea intutiva dietro le statistiche sufficienti è quella di condensare l'informazione
del campione aleatorio conservando quello che è utile per risalire al valore "vero"
del parametro.
Sembra quindi inuitivo che gli stimatori efficienti debbano essere costruiti solo a partire da statistiche sufficienti.
Questa idea è resa rigorosa
dal teorema di Rao-Blackwell; prima di enunciarlo, dobbiamo ricordare alcune
nozioni sul valor medio condizionato, restringendoci solamente al caso discreto.
Siano X Y valori aleatorie discrete con momento secondo finito e funzione
di probabilità congiunta $P_{XY}(x,y)$;
la densità di X condizionata a $Y=y$ è data
da
\[
p_{X|Y}(x,y)=\begin{cases}\frac{p_{XY}(x,y)}{p_{Y}(y)}, & se~p_{Y}(y)>0\\ 0 & altrimenti\end{cases}
\]
(Nota: il testo originale riporta $p_X(x)$ al denominatore, ma è $p_Y(y)$)

ed il valor medio condizionato a $Y=y$ naturalmente
\[
E[X|Y=y] = \sum_{x \in Dom(X)} x p_{X|Y}(x,y)
\]
(Nota: il testo originale presenta una formula parziale)

Se evitiamo di fissare il valore della variabile aleatoria Y, avremo ora una nuova
variabile aleatoria (funzione di Y) della forma $\psi(Y)=E[X|Y],$ che ad ogni
valore di Y associa il valore del valor medio condizionato corrispondente a quel
valore
\[
E[X|Y]=\sum_{x\in Dom(X)}x\frac{p_{XY}(x,Y)}{p_{Y}(Y)}=\sum_{x\in Dom(X)}xp_{X|Y}(x|Y) ;
\]
in altre parole, $\psi(Y)$ assume il valore $\psi(y)=E[X|Y=y]$ con probabilità $p_{Y}(y)$
(la legge marginale di Y. E' immediato verificare che (la cosiddetta legge del
valor medio iterato)
\[
E[E[X|Y]] = E[X];
\]
infatti
\begin{align*}
E[E[X|Y]] &= \sum_{y}E[X|Y=y]p_{Y}(y)=\sum_{y}\sum_{x\in Dom(X)}x\frac{p_{XY}(x,y)}{p_{Y}(y)}p_{Y}(y) \\
&= \sum_{x\in Dom(X)}x\sum_{y}p_{XY}(x,y) \\
&= \sum_{x\in Dom(X)}xp_{X}(x)=E[X] .
\end{align*}
Allo stesso modo possiamo definire la variabile aleatoria (funzione di Y)
\[
Var[X|Y]=\sum_{x\in Dom(X)}(x-E[X|Y])^{2}p_{X|Y}(x|Y) ,
\]
che corrisponde alla varianza condizionata di X, in funzione del valore aleatorio
di Y. Vale il seguete risultato.

\begin{lemma}
Siano X,Y variabili aleatorie di quadrato integrabile definite sullo
stesso spazio di probabilità. Abbiamo
\[
Var[X]=E[Var[X|Y]]+Var[E[X|Y]].
\]
\end{lemma}

\begin{proof}
Abbiamo che
\begin{align*}
Var[X] &= E[(X-E[X])^{2}] \\
&= E[(X-E[X|Y]+E[X|Y]-E[X])^{2}] \\
&= E[(X-E[X|Y])^{2}]+E[(E[X|Y]-E[X])^{2}] \\
&\quad + 2E[(X-E[X|Y])(E[X|Y]-E[X])].
\end{align*}
Ora per definizione abbiamo
\[
E[(E[X|Y]-E[X])^{2}]=E_{Y}[(E[X|Y]-E[X])^{2}]=Var[E[X|Y]];
\]
inoltre
\begin{align*}
E[(X-E[X|Y])(E[X|Y]-E[X])] &= E_{Y}[E[(X-E[X|Y])(E[X|Y]-E[X])|Y]] \\
&= E_{Y}[(E[X|Y]-E[X])E[(X-E[X|Y])|Y]] \\
&= 0 ,
\end{align*}
perché $E[(X-E[X|Y])|Y]=0.$ Infine
\[
E[(X-E[X|Y])^{2}]=E[E[(X-E[X|Y])^{2}|Y]]=E[Var[X|Y]],
\]
e il Lemma è dimostrato.
\end{proof}

Possiamo finalmente arrivare al risultato principale, che ci garantisce che
condizionando uno stimatore non distorto su una statistica sufficiente se ne
ottiene un altro ancora non distorto e con varianza non maggiore.

\begin{theorem}[Rao-Blackwell]
Sia $W_{n}=W_{n}(X_{1},...,X_{n})$ uno stimatore non
distorto e con varianza finita del parametro $\theta$, e sia $T_{n}=T(X_{1},...,X_{n})$ una
statistica sufficente per $\theta$. Definiamo $\psi(T)=E[W_{n}|T]$;
allora abbiamo
\[
E[\psi(T)]=\theta~e~Var[\psi(T)]\le Var[W_{n}]
\]
\end{theorem}

\begin{proof}
La dimostrazione è una applicazione immediata dei risultati precedenti
sul valor medio condizionato. In particolare
\[
E[\psi(T)]=E[E[W_{n}|T]]=E[W_n]=\theta ,
\]
\[
Var[W_{n}]=E[Var[W_{n}|T]]+Var[E[W_{n}|T]]
\]
\[
\ge Var[E[W_{n}|T]]=Var[\psi(T)].
\]
\end{proof}

\begin{remark}
Dove abbiamo usato nella dimostrazione precedente la sufficienza?
Apparentemente non gioca alcun ruolo, ma in realtà ci garantisce che il valor
medio condizionato non dipenda dal parametro, e quindi che $\psi(T)$ sia effettivamente uno stimatore.
Ad esempio consideriamo un campione di sole due
osservazioni $X_{1}$, $X_{2}$ Gaussiane indipendenti con valor medio $\mu$ e varianza 1, e
consideriamo lo stimatore $\overline{X}_{2}=\frac{1}{2}(X_{1}+X_{2})$.
Consideriamo
\[
\psi(X_{1}):=E[\overline{X_{2}}|X_{1}]=\frac{X_{1}+\mu}{2}
\]
Questa variabile aleatoria ha valor medio $\mu$ e varianza $\frac{1}{4}<Var(\overline{X}_{2})=\frac{1}{2}$; non
si tratta però di uno stimatore, perchè il suo valore non può essere determinato
senza conoscere il valore del parametro $\mu$.
\end{remark}

\end{document}