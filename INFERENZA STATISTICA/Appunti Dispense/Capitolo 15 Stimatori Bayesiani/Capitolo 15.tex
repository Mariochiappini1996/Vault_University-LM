\documentclass[article,a4paper]{article}

% --- PACHETTI ESSENZIALI ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[italian]{babel}
\usepackage[a4paper, margin=1in]{geometry} % Imposta i margini della pagina

% --- PACHETTI PER LA MATEMATICA ---
\usepackage{amsmath}
\usepackage{amssymb} % Per \mathbb
\usepackage{amsthm} % Per gli ambienti di proposizione e proof
\usepackage{mathrsfs} % Per \mathfrak

% --- DEFINIZIONE DEGLI AMBIENTI ---
\newtheoremstyle{miostile}
  {\topsep} % space before
  {\topsep} % space after
  {\itshape} % body font
  {} % indent
  {\bfseries} % head font
  {.} % punctuation after head
  {.5em} % space after head
  {} % head spec
\theoremstyle{miostile}

% Imposta la numerazione per continuare dai capitoli precedenti
\newtheorem{theorem}{Theorem}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{exercise}[theorem]{Exercise}
\setcounter{theorem}{122} % L'ultimo era 122, quindi il prossimo è 123

% Rridefiniamo l'ambiente proof in italiano
\renewcommand{\proofname}{Proof}

% Comandi personalizzati
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}


% --- INIZIO DEL DOCUMENTO ---
\begin{document}

\section*{15 Stimatori Bayesiani}
Ricordiamo innanzitutto la Formula di Bayes:

\begin{theorem}[Bayes]
Siano $H_{1},...H_{m}$ eventi disgiunti ed esaustivi, cioè $H_{i}\cap$
$H_{j}=\emptyset$ per ogni $i\ne j$ e $\cup_{i=1}^{m} H_i = \Omega.$ Sia inoltre $E\in\mathfrak{S}$ un evento con probabilità
strettamente positiva;
allora
\[
Pr(H_{i}|E)=\frac{Pr(E|H_{i})Pr(H_{i})}{\sum_{j=1}^{m}Pr(E|H_{j})Pr(H_{j})}.
\]
\end{theorem}

La derivazione della formula di Bayes è matematicamente banale (si riduce
essenzialmente a ricordare che $Pr(E)=\sum_{j=1}^{m}Pr(E|H_{j})Pr(H_{j})$; l'interpretazione
però è molto importante, perché permette di combinare in modo matematicamente la probabilità a priori di m cause disgiunte $H_{j}$ e l'evidenza empirica sul
fatto che E si sia verificato.

\begin{example}
L'approccio Bayesiano all'inferenza statistica è profondamente diverso da
quello che abbiamo seguito finora.
L'idea di fondo è che non esista un "vero"
parametro da stimare $\theta=\theta_{0}$, ma che il parametro sia esso stesso una variabile
(o un vettore) aleatorio la cui distribuzione, che rappresenta il nostro stato
di conoscenza, viene aggiornata tramite la formula di Bayes alla luce delle osservazioni.
In altre parole, oltre alla legge delle osservazioni $f(X_{1},...,X_{n}|\theta)$
dobbiamo supporre di conoscere la legge $\pi(\theta)$ del parametro prima di aver effettuato osservazioni.
Si noti come abbiamo scritto $f(X_{1},...,X_{n}|\theta)$ invece di
$f(X_{1},...,X_{n};\theta)$ perché ora ha senso parlare della legge di $X_{1},...,X_{n}$ condizionatamente al valore $\theta$ del parametro.
L'oggetto centrale dell'inferenza diviene
quindi la legge a posteriori, che attraverso la formula di Bayes è data da
\[
\pi(\theta|X_{1},...,X_{n})=\frac{f(X_{1},...,X_{n}|\theta)\pi(\theta)}{\int_{\Theta}f(X_{1},...,X_{n}|\theta)\pi(\theta)d\theta}
\]
ed analogamente nel caso discreto.
\end{example}

\begin{example}
Consideriamo $X_{1},...,X_{n}$ variabili Bernoulliane di parametro p;
per quest'ultimo, assumiamo che abbia una distribuzione a priori di tipo Beta
con parametri $\alpha$, $\beta$, cioè
\[
\pi(p)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{\alpha-1}(1-p)^{\beta-1}, \quad p\in[0,1].
\]
Ricordiamo innanzitutto i valori di valor medio e varianza
\[
E[p]=\frac{\alpha}{\alpha+\beta} \quad Var[p]=\frac{\alpha\beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}
\]
infatti
\begin{align*}
E[p] &= \int_{0}^{1}p\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{\alpha-1}(1-p)^{\beta-1}dp \\
&= \frac{\Gamma(\alpha+1)\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\alpha+\beta+1)}\int_{0}^{1}\frac{\Gamma(\alpha+\beta+1)}{\Gamma(\alpha+1)\Gamma(\beta)}p^{\alpha}(1-p)^{\beta-1}dp \\
&= \frac{\alpha}{\alpha+\beta}
\end{align*}
e
\begin{align*}
E[p^{2}] &= \int_{0}^{1}p^{2}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{\alpha-1}(1-p)^{\beta-1}dp \\
&= \frac{\Gamma(\alpha+2)\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\alpha+\beta+2)}\int_{0}^{1}\frac{\Gamma(\alpha+\beta+2)}{\Gamma(\alpha+2)\Gamma(\beta)}p^{\alpha+1}(1-p)^{\beta-1}dp \\
&= \frac{\alpha(\alpha+1)}{(\alpha+\beta+1)(\alpha+\beta)}
\end{align*}
(Nota: il testo originale $p^\alpha$ nell'integrale di $E[p^2]$ è stato corretto a $p^{\alpha+1}$ per coerenza con la formula)
\[
Var[p]=\frac{\alpha(\alpha+1)}{(\alpha+\beta+1)(\alpha+\beta)}-\frac{\alpha^{2}}{(\alpha+\beta)^{2}}
\]
\[
=\frac{\alpha(\alpha+1)(\alpha+\beta)-\alpha^{2}(\alpha+\beta+1)}{(\alpha+\beta+1)(\alpha+\beta)^{2}}=\frac{\alpha\beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}
\]
Abbiamo inoltre, scrivendo $y=\sum_{i=1}^{n}X_{i}$
\begin{align*}
f(y|p)\pi(p) &= \binom{n}{y}p^{y}(1-p)^{n-y}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{\alpha-1}(1-p)^{\beta-1} \\
&= \binom{n}{y}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{y+\alpha-1}(1-p)^{n-y+\beta-1}.
\end{align*}
La marginale a denominatore si ottiene come segue:
\begin{align*}
f(y) &= \int_{0}^{1}\binom{n}{y}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{y+\alpha-1}(1-p)^{n-y+\beta-1}dp \\
&= \binom{n}{y}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\frac{\Gamma(y+\alpha)\Gamma(n-y+\beta)}{\Gamma(n+\alpha+\beta)}
\end{align*}
Infine la distribuzione a posteriori è data da
\begin{align*}
\pi(p|y) &= \frac{\binom{n}{y}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{y+\alpha-1}(1-p)^{n-y+\beta-1}}{\binom{n}{y}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\frac{\Gamma(y+\alpha)\Gamma(n-y+\beta)}{\Gamma(n+\alpha+\beta)}} \\
&= \frac{\Gamma(n+\alpha+\beta)}{\Gamma(y+\alpha)\Gamma(n-y+\beta)}p^{y+\alpha-1}(1-p)^{n-y+\beta-1},
\end{align*}
cioè è ancora una beta, con parametri aggiornati. Quando la distribuzioni a
posteriori assume la stessa forma dell'a priori si parla di leggi coniugate.
\end{example}

\begin{remark}
Una interpretazione rigorosa dell'approccio Bayesiano dovrebbe
concludersi con la derivazione della distribuzione a posteriori: il calcolo di uno
stimatore "puntuale "non ha strettamente senso, visto che il paramrto non ha un
singolo valore.
In pratica però il calcolo degli stimatori Bayesiani si conclude
molto spesso con la derivazione di un singolo valore di sintesi, come ad esempio
il valore che massimizza la distribuzione a posteriori o ancora più spesso il valore
medio a posteriori.
Nel caso della binomiale con a priori beta otteniamo
\begin{align*}
\hat{p}_{Bayes} &= \int_{0}^{1}p\frac{\Gamma(n+\alpha+\beta)}{\Gamma(y+\alpha)\Gamma(n-y+\beta)}p^{y+\alpha-1}(1-p)^{n-y+\beta-1}dp \\
&= \frac{y+\alpha}{n+\alpha+\beta}=\frac{y}{n}\frac{n}{n+\alpha+\beta}+\frac{\alpha}{\alpha+\beta}\frac{\alpha+\beta}{n+\alpha+\beta}
\end{align*}
L'ultima espressione è illuminante, perchè rappresenta lo "stimatore Bayesiano"
come una media ponderata di due elementi: lo stimatore classico di massima
verosimiglianza (in questo caso, la semplice media aritmetica) ed il valore atteso
a priori:
\[
\frac{y}{n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}=\overline{X}_{n},\frac{\alpha}{\alpha+\beta}
\]
Il peso dello stimatore di massima verosimiglianza converge ad 1 quando la
dimensione del campione n diverge all'infinito;
intuitvamente, le nostre opinioni
a priori sono schiacciate dalla forza dell'evidenza empirica.
\end{remark}

\begin{exercise}
Sia X una Gaussiana con valor medio $\theta$ e varianza $\sigma^{2}$, con $\theta$
essa stessa Gaussiana di parametri $\mu,\tau^{2}$.
Abbiamo che
\[
\pi(\theta|X)\sim N(\frac{\tau^{2}}{\tau^{2}+\sigma^{2}}X+\frac{\sigma^{2}}{\sigma^{2}+\tau^{2}}\mu,\frac{\sigma^{2}\tau^{2}}{\sigma^{2}+\tau^{2}}) .
\]
\end{exercise}

\end{document}