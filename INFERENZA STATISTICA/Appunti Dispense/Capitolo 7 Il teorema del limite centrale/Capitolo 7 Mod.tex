\documentclass[article,a4paper]{article}

% --- PACHETTI ESSENZIALI ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[italian]{babel}
\usepackage[a4paper, margin=1in]{geometry} % Imposta i margini della pagina

% --- PACHETTI PER LA MATEMATICA ---
\usepackage{amsmath}
\usepackage{amssymb} % Per \mathbb
\usepackage{amsthm} % Per gli ambienti di proposizione e proof
\usepackage{mathrsfs} % Per \mathfrak
\usepackage{cases} % Per l'ambiente cases

% --- PACCHETTO PER LE SPIEGAZIONI (TCOLORBOX) ---
\usepackage[skins,breakable]{tcolorbox}

% --- DEFINIZIONE DEGLI AMBIENTI ---
\newtheoremstyle{miostile}
  {\topsep} % space before
  {\topsep} % space after
  {\itshape} % body font
  {} % indent
  {\bfseries} % head font
  {.} % punctuation after head
  {.5em} % space after head
  {} % head spec
\theoremstyle{miostile}

% Imposta la numerazione per continuare dai capitoli precedenti
\newtheorem{example}{Example}
\newtheorem{theorem}[example]{Theorem}
\newtheorem{condition}[example]{Condition}
\newtheorem{remark}[example]{Remark}
\setcounter{example}{63} % L'ultimo era 63, quindi il prossimo è 64

% Rridefiniamo l'ambiente proof in italiano
\renewcommand{\proofname}{Proof}

% Comandi personalizzati
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}


% --- STILE PER LE SCATOLE DI SPIEGAZIONE ---
% Definiamo un nuovo ambiente tcolorbox chiamato 'spiegazione'
\tcbset{
    spiegazionestyle/.style={
        colback=gray!10, % Sfondo grigio chiaro
        colframe=gray!60, % Bordo grigio scuro
        fonttitle=\bfseries,
        title=Spiegazione Semplice,
        sharp corners,
        boxsep=5pt,
        left=5pt,
        right=5pt,
        top=5pt,
        bottom=5pt,
        breakable, % Permette al box di spezzarsi tra le pagine
    }
}
% Creiamo il comando \begin{spiegazione} ... \end{spiegazione}
\newtcolorbox{spiegazione}{spiegazionestyle}


% --- INIZIO DEL DOCUMENTO ---
\begin{document}

\section*{7 Il teorema del limite centrale}
La formulazione più semplice del teorema limite centrale che si incontra in un
primo corso di probabilità è la seguente.

\begin{theorem}
Sia $X_{1},...,X_{n}$ una seuenza di variabili aleatorie indipendenti ed
identicamente distribuite con valor medio $\mu$ e varianza (finita) $\sigma^{2}$. Allora abbiamo la convergenza in distribuzione
\[
\frac{\sum_{i=1}^{n}X_{i}-n\mu}{\sqrt{n\sigma^{2}}}\rightarrow_{d}Z\sim N(0,1) , \text{ per } n\rightarrow\infty
\]
\end{theorem}

\begin{spiegazione}
    \textbf{Questo è il Teorema del Limite Centrale (CLT)}
    
    Questa è forse la formula più importante di tutta la statistica.
    
    \textbf{Cosa dice in parole povere?}
    Prendi \textit{qualsiasi} variabile aleatoria $X$ (es. il lancio di un dado, che ha una distribuzione "piatta"). Non importa quale sia la sua forma originale, purché abbia una media $\mu$ e una varianza $\sigma^2$ finite.
    
    Ora, prendi un campione di $n$ osservazioni ($X_1, \dots, X_n$) e calcola la loro \textbf{somma} $\sum X_i$.
    
    Il teorema dice che, per $n$ grande, la \textit{distribuzione} di questa somma (o della sua media), una volta "standardizzata", assomiglierà sempre e comunque a una Gaussiana Standard $N(0,1)$ (la "curva a campana").
    
    \textbf{Cosa significa "Standardizzare"?}
    È un'operazione di "zoom e centratura" per guardare la distribuzione alla giusta scala:
    \begin{enumerate}
        \item \textbf{$\sum X_i - n\mu$}: Centriamo la somma sottraendo il suo valore medio.
        \item \textbf{$/ \sqrt{n\sigma^2}$}: Riduciamo la scala (dividendo per la sua deviazione standard) per evitare che la curva si "allarghi" o "schiacci" all'infinito.
    \end{enumerate}
    
    Questo teorema ci dà il permesso di usare la curva Normale (Gaussiana) per fare stime e test d'ipotesi su quasi tutto, purché abbiamo abbastanza dati ($n$ grande).
\end{spiegazione}

\begin{proof}
Ricordiamo innanzitutto la definizione di funzione caratteristica di una
variabile aleatoria X:
\[
\psi_{X}(t):=E[exp(iXt)].
\]
Ricordiamo altri due fatti dai corsi elementari di probabilità: Il Teorema di
continuità di Lévy ci garantisce che se la sequenza di funzioni di caratteristiche
$\psi_{X_{n}}(t)=E[exp(iX_{n}t)]$ converge a $\psi_{X}(t)=E[exp(iXt)]$, necessariamente si ha
la convergenza in distribuzione $X_{n}\rightarrow_{d}X$ (eviceversa). Ovviamente nel nostro
caso è sufficiente dimostrare che
\[
\frac{\sum_{i=1}^{n}\tilde{X}_{i}}{\sqrt{n}}\rightarrow_{d}Z\sim N(0,1) , \text{ per } n\rightarrow\infty, \text{ con } \tilde{X_{i}}:=\frac{X_{i}-\mu}{\sigma}
\]
Abbiamo dunque, usando le proprietà standard della funzione caratteristica
\begin{align*}
\psi_{\frac{\sum_{i=1}^{n}\tilde{X}_{i}}{\sqrt{n}}}(t) &= E[\exp(i\sum_{i=1}^{n}\tilde{X}_{i}\frac{t}{\sqrt{n}})] \\
&= \psi_{\sum_{i=1}^{n}\tilde{X}_{i}}(\frac{t}{\sqrt{n}}) \\
&= \psi_{\tilde{X}_{1}}(\frac{t}{\sqrt{n}})\times...\times\psi_{\tilde{X}_{n}}(\frac{t}{\sqrt{n}}) \\
&= \{\psi_{\tilde{X}_{1}}(\frac{t}{\sqrt{n}})\}^{n}
\end{align*}
dove nel secondo passaggio abbiamo usato il fatto che le variabili sono indipendenti e nel terzo il fatto che sono identicamente distribuite.
Ricordiamo ora
che
\[
\psi_{\tilde{X}_{1}}(t)=1+itE[\tilde{X}_{1}]+\frac{(it)^{2}}{2}E[\tilde{X}_{1}^{2}]+o((it)^{2}),
\]
da cui
\begin{align*}
\{\psi_{\tilde{X}_{1}}(\frac{t}{\sqrt{n}})\}^{n} &= \{1+i\frac{t}{\sqrt{n}}E[\tilde{X}_{1}]+\frac{(it)^{2}}{2n}E[\tilde{X}_{1}^{2}]+o((\frac{it}{\sqrt{n}})^{2})\}^{n} \\
&= \{1-\frac{t^{2}}{2n}+o(\frac{t^{2}}{n})\}^{n}\rightarrow \exp(-\frac{t^{2}}{2}).
\end{align*}
La dimostrazione è conclusa ricordando che la Gaussiana standard ha proprio
funzione caratteristica
\[
\psi_{Z}(t)=\exp(-\frac{t^{2}}{2}) .
\]
\end{proof}

\begin{spiegazione}
    \textbf{Come si dimostra il Teorema (l'idea)?}
    
    La dimostrazione formale è tecnica, ma l'idea è elegante e si basa su uno strumento chiamato \textbf{Funzione Caratteristica} $\psi_X(t)$.
    
    \begin{itemize}
        \item \textbf{Cos'è?} È una "impronta digitale" (o il "DNA") di una distribuzione di probabilità. Ogni distribuzione ne ha una unica, e viceversa. È l'equivalente della Trasformata di Fourier per la probabilità.
        
        \item \textbf{Perché si usa?} Ha una proprietà magica: la funzione caratteristica della \textit{somma} di variabili indipendenti è semplicemente il \textit{prodotto} delle loro funzioni caratteristiche. (Sommare variabili = Moltiplicare le loro "impronte").
        
        \item \textbf{Il Teorema di Lévy:} Questo teorema (menzionato nel testo) collega la convergenza delle distribuzioni (quella che ci interessa, $\to_d$) alla convergenza delle loro "impronte digitali" ($\psi_{X_n}(t) \to \psi_X(t)$).
    \end{itemize}
    
    La dimostrazione, quindi, consiste nel:
    \begin{enumerate}
        \item Calcolare la funzione caratteristica della variabile standardizzata (che è una somma).
        \item Usare la "proprietà magica" per trasformare la somma in un prodotto, che diventa una potenza $(\dots)^n$.
        \item Usare lo sviluppo di Taylor (un'approssimazione) per la funzione all'interno della parentesi.
        \item Usare il limite notevole $(1 + a/n)^n \to e^a$ per dimostrare che l'intera espressione converge a $e^{-t^2/2}$.
        \item Riconoscere che $e^{-t^2/2}$ è l'"impronta digitale" della Gaussiana Standard $N(0,1)$.
        \item Invocare il Teorema di Lévy per dire: "Se le impronte digitali convergono, allora anche le distribuzioni convergono".
    \end{enumerate}
\end{spiegazione}

Nell'ambito di questo corso, avremo bisogno di un Teorema del Limite Centrale di portata ben più generale.
A tale fine, introduciamo la cosiddetta condizione di Lindeberg.

\begin{condition}[Lindeberg]
Sia $\{X_{i}\}$ una successione di variabili inidpendenti
(non necessariamente identicamente distribuite) definite su uno spazio di probabilità $\{\Omega, \mathfrak{S}, \mathbb{P}\}$ e con momento secondo finito;
scriviamo $\mu_{i}=E[X_{i}]$ e $\sigma_{i}^{2}=$
$Var[X_{i}]$. Questa sequenza soddisfa la condizione di Lindeberg se per ogni $\epsilon>0$
si ha
\[
\lim_{n\rightarrow\infty}\frac{1}{\sum_{j=1}^{n}\sigma_{j}^{2}}\sum_{i=1}^{n}E[(X_{i}-\mu_i)^{2}I_{\{|X_{i}-\mu_i|>\epsilon\sqrt{\sum_{j=1}^{n}\sigma_{j}^{2}}\}}]=0 .
\]
(Nota: il testo originale $E[X_{i}^{2}I_{\{X_{i}^{2}>\epsilon\sqrt{\sum_{i=1}\sigma_{i}^{2}}\}}]$ è stato corretto con la forma standard della condizione, che è più chiara).
\end{condition}

\begin{spiegazione}
    \textbf{Perché un altro Teorema? (La condizione di Lindeberg)}
    
    Il Teorema 64 è ottimo, ma ha un'ipotesi molto restrittiva: "identicamente distribuite" (i.i.d.). Significa che tutte le $X_i$ devono provenire dalla stessa identica distribuzione (es. tutti lanci dello \textit{stesso} dado).
    
    E se sommiamo cose \textit{diverse}? (es. $X_1$ è un dado, $X_2$ è una moneta, $X_3$ è l'altezza di una persona...). Il Teorema di Lindeberg (Teorema 66) è un CLT \textit{molto più generale} che funziona anche per variabili \textbf{non} identicamente distribuite.
    
    \textbf{Cosa significa la formula?}
    La formula di Lindeberg è intimidatoria. In parole povere, è una condizione tecnica che assicura che nessuna \textit{singola} variabile $X_i$ sia "troppo importante" e domini la somma.
    
    La somma $\sum (X_i - \mu_i)$ deve essere il risultato di \textit{tanti piccoli} pezzi casuali. Se ci fosse un pezzo (una $X_k$) la cui varianza $\sigma_k^2$ fosse enorme e "schiacciasse" la somma di tutte le altre, il risultato non sarebbe Gaussiano.
    
    La condizione di Lindeberg garantisce che il contributo dei valori "estremi" (le code) di tutte le variabili sia trascurabile rispetto alla varianza totale della somma.
\end{spiegazione}

\begin{theorem}
Sia $\{X_{i}\}$ una successione di variabili aleatorie che soddisfa la
condizione di Lindeberg. Allora
\[
\frac{\sum_{i=1}^{n}X_{i}-\sum_{i=1}^{n}\mu_{i}}{\sqrt{\sum_{i=1}^{n}\sigma_{i}^{2}}}\rightarrow_{d}Z\sim N(0,1) .
\]
\end{theorem}

\begin{remark}
La dimostrazione di questo risultato si basa su approssimazioni
successive della funzione caratteristica e non è riportata per brevità.
E' però interessante capire il significato della condizione di Lindeberg, che intuitivamente
tende ad escludere due casi:
\begin{itemize}
    \item[a)] quello in cui la somma delle varianza NON diverga, cioè $\sum_{i=1}\sigma_{i}^{2}$ vada ad
una costante.
In questo caso stiamo stia in pratica sommando solo un numero finito
di variabili aleatorie e pertanto il TLC non può valere, a meno che le variabili
non siano già in partenza Gaussiane
    \item[b)] quello in cui la varianza nella coda delle ultime variabili aleatorie sommate
sia dello stesso ordine di grandezza di tutte le altre;
anche in questo caso il
TLC non può valere perché è come se sommassimo due singole variabili, una
identificata dalla somma delle prime $n-1$ e l'altra costituita dall'ultima.
\end{itemize}
\end{remark}

\begin{remark}
La condizione di Lindeberg è difficile da verificare in pratica e
si usa più spesso una meno generale, ma di più facile verifica, la cosiddetta
condizione di Lyapunov.
\end{remark}

\begin{condition}[Lyapunov]
Sia $\{X_{i}\}$ una successione di variabili aleatori indipendenti ed a media nulla e definiamo
\[
\tilde{X}_{i,n}=\frac{X_{i}}{\sqrt{\sum_{j=1}^{n}E[X_{j}^{2}]}}
\]
in modo tale che $\sum_{i=1}^{n}E[\tilde{X}_{i,n}^{2}]=1$ per ogni n. La sequenza soddisfa la condizione
di Lyapunov se
\[
\lim_{n\rightarrow\infty}\sum_{i=1}^{n}E[|\tilde{X}_{i,n}|^{2+\delta}]=0 \text{ per qualche } \delta>0.
\]
(Nota: il testo originale $\lim_{n\rightarrow\infty}max_{i=1,...,n}E[|\tilde{X}_{i,n}|^{2+\delta}]=0$ è una versione, questa con la somma è un'altra forma comune).
\end{condition}

\begin{spiegazione}
    \textbf{Perché ancora un'altra condizione? (Lyapunov)}
    
    La condizione di Lindeberg (65) è la condizione teorica perfetta (è quasi "necessaria e sufficiente"), ma è molto difficile da verificare in pratica.
    
    La condizione di Lyapunov è una scorciatoia. È una condizione \textit{più forte} di Lindeberg (cioSè, se vale Lyapunov, vale automaticamente anche Lindeberg), ma è molto \textit{più facile} da calcolare.
    
    In pratica, se riesci a dimostrare che un "momento" leggermente superiore al secondo (come $2+\delta$) va a zero quando normalizzato, hai dimostrato che vale il CLT. È uno strumento pratico per i matematici.
\end{spiegazione}

\begin{remark}
Anche la condizione di Lyapunov può essere vista come la richiesta
che non ci sia nessuna variabile aleatoria con varianza non trascurabile rispetto
alla somma di tutte le altre..
\end{remark}

\begin{remark}
E' importante discutere la relazione esistente tra teorema del limite centrale e legge dei garndi numeri.
Consideriamo il caso di variabili indipendenti $X_{i}$, e normalizziamole in modo che abbiano valor medio nullo e
varianza 1. Sappiamo che la somma di questa variabili aleatorie ha varianza
$Var[\sum_{i=1}^{n}X_{i}]=n,$ e pertanto $\sum_{i=1}^{n}X_{i}=O_{p}(\sqrt{n})$;
in altre parole questa sequenza di somme se divisa per $\sqrt{n}$ vivrà su un compatto con probabilità che può
essere resa arbitrariamente vicina ad uno.
la legge dei grandi numeri ci dice
che dividendo per n abbiamo
\[
\overline{X}_{n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}\rightarrow_{p}0
\]
in un certo senso, il Teorema del Limite Centrale si può pensare come uno
zoom sulle fluttuazioni che diventano infinitesimali della media intorno allo zero:
riscalando per un fattore $\sqrt{n}$ otteniamo
\[
\sqrt{n}\overline{X}_{n}=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}X_{i}\rightarrow_{d}N(0,1)
\]
.
Quindi le fluttuazioni nella somma di n variabili aleatorie indipendenti a valor
medio nullo sono di ordine $\sqrt{n};$ le fluttuazioni della media aritmetica sono di
ordine $\frac{1}{\sqrt{n}}$ le fluttuazioni nel teorema del limite centrale sono di ordine $O(1)$.
\end{remark}

\begin{spiegazione}
    \textbf{Legge dei Grandi Numeri (LLN) vs. Teorema Limite Centrale (CLT)}
    
    Questo è un punto cruciale che riassume la differenza tra i due teoremi più importanti:
    
    \begin{enumerate}
        \item \textbf{Legge dei Grandi Numeri (LLN):} Ci dice \textit{dove} la media $\overline{X}_n$ converge.
        \[
        \overline{X}_n \rightarrow_{p} \mu
        \]
        La media campionaria "collassa" sulla media vera $\mu$. L'errore ($\overline{X}_n - \mu$) va a zero.
        
        \item \textbf{Teorema del Limite Centrale (CLT):} Ci dice \textit{come} la media $\overline{X}_n$ fluttua \textit{mentre} converge.
        La LLN ci dice che $\overline{X}_n$ si avvicina a $\mu$. L'errore ($\overline{X}_n - \mu$) è di ordine $O_p(1/\sqrt{n})$, cioè diventa molto piccolo.
        
        Il CLT fa uno \textbf{"zoom"} su questo errore piccolissimo: moltiplica l'errore per $\sqrt{n}$ per "riportarlo in scala".
        \[
        \sqrt{n}(\overline{X}_n - \mu) \rightarrow_{d} N(0, \sigma^2)
        \]
        Ci dice che questo errore "ingrandito" non va a zero, ma si stabilizza in una perfetta curva Gaussiana.
    \end{enumerate}
    
    La LLN ci dà la destinazione, il CLT ci dà la "forma" del percorso e la velocità di convergenza.
\end{spiegazione}

\begin{example}
(L'uso del Teorema del limite centrale nei sondaggi di opinione.)...
\end{example}

\end{document}